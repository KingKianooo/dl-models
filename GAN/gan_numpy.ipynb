{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bc48bc3",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce58fad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kagglehub\n",
      "  Downloading kagglehub-0.4.1-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting kagglesdk<1.0,>=0.1.14 (from kagglehub)\n",
      "  Downloading kagglesdk-0.1.15-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in /Users/king/miniconda3/lib/python3.13/site-packages (from kagglehub) (24.2)\n",
      "Requirement already satisfied: pyyaml in /Users/king/miniconda3/lib/python3.13/site-packages (from kagglehub) (6.0.2)\n",
      "Requirement already satisfied: requests in /Users/king/miniconda3/lib/python3.13/site-packages (from kagglehub) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /Users/king/miniconda3/lib/python3.13/site-packages (from kagglehub) (4.67.1)\n",
      "Requirement already satisfied: protobuf in /Users/king/miniconda3/lib/python3.13/site-packages (from kagglesdk<1.0,>=0.1.14->kagglehub) (6.31.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/king/miniconda3/lib/python3.13/site-packages (from requests->kagglehub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/king/miniconda3/lib/python3.13/site-packages (from requests->kagglehub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/king/miniconda3/lib/python3.13/site-packages (from requests->kagglehub) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/king/miniconda3/lib/python3.13/site-packages (from requests->kagglehub) (2025.11.12)\n",
      "Downloading kagglehub-0.4.1-py3-none-any.whl (69 kB)\n",
      "Downloading kagglesdk-0.1.15-py3-none-any.whl (160 kB)\n",
      "Installing collected packages: kagglesdk, kagglehub\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [kagglehub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed kagglehub-0.4.1 kagglesdk-0.1.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/king/miniconda3/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading to /Users/king/.cache/kagglehub/datasets/jeffheaton/glasses-or-no-glasses/2.archive...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6.11G/6.11G [03:41<00:00, 29.7MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import kagglehub\n",
    "\n",
    "# Download latest version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf672bb",
   "metadata": {},
   "source": [
    "# Random Noise Sampler\n",
    "\n",
    "- Implement a function to generate random noise vectors (latent codes) zz which serve as input to the Generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f04a17ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_noise_sampler(batch_size: int, latent_dim: int, mode: str = 'gaussian', seed: int = None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generates a batch of random noise vectors.\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    if mode == 'gaussian':\n",
    "        samples = np.random.normal(0.0, 1.0, size=(batch_size, latent_dim))\n",
    "    elif mode == 'uniform':\n",
    "        samples = np.random.uniform(-1.0, 1.0, size=(batch_size, latent_dim))\n",
    "    else:\n",
    "        raise ValueError(\"mode must be 'gaussian' or 'uniform'\")\n",
    "\n",
    "    return samples.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2c28e7",
   "metadata": {},
   "source": [
    "# Generator Forward Pass (Linear)\n",
    "\n",
    "- Implement the forward pass of a Generator network using only linear (fully-connected) layers and activation functions.\n",
    "- \n",
    "    The first N−1N−1 layers use ReLU activation: hnew=ReLU(holdW+b)hnew​=ReLU(hold​W+b).\n",
    "    The final layer uses Tanh activation: y=Tanh(holdW+b)y=Tanh(hold​W+b).\n",
    "\n",
    "- \n",
    "    z: Latent input vectors of shape (batch_size, input_dim).\n",
    "    weights: A list of weight matrices [W_0, W_1, ..., W_{N-1}].\n",
    "        Each WiWi​ has shape (in_dim, out_dim).\n",
    "    biases: A list of bias vectors [b_0, b_1, ..., b_{N-1}].\n",
    "        Each bibi​ has shape (out_dim,).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082a2fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_forward(z: np.ndarray, weights: list[np.ndarray], biases: list[np.ndarray]) -> tuple[np.ndarray, list[np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Computes the forward pass of the generator.\n",
    "     \n",
    "    Args:\n",
    "        z: Input latent vectors (batch_size, latent_dim).\n",
    "        weights: List of weight matrices.\n",
    "        biases: List of bias vectors.\n",
    "        \n",
    "    Returns:\n",
    "        output: Final generated data.\n",
    "        activations: List of activations [z, h1, ..., output].\n",
    "    \"\"\"\n",
    "    activations = [z]\n",
    "    h = z\n",
    "\n",
    "    N = len(weights)\n",
    "\n",
    "    for i in range(N):\n",
    "        h = h @ weights[i] + biases[i]\n",
    "         \n",
    "        if i < N - 1:\n",
    "            # First N−1 layers → ReLU\n",
    "            h = np.maximum(0, h)\n",
    "        else:\n",
    "            # Final layer → Tanh\n",
    "            h = np.tanh(h)\n",
    "\n",
    "        activations.append(h)\n",
    "\n",
    "    return h, activations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41a41e8",
   "metadata": {},
   "source": [
    "# Discriminator Forward Pass (Linear)\n",
    "\n",
    "- The discriminator consists of NN layers (typically small, e.g., 2 layers for simple problems).\n",
    "\n",
    "    The first N−1N−1 layers use LeakyReLU activation with slope 0.20.2: hnew=LeakyReLU(holdW+b)hnew​=LeakyReLU(hold​W+b).\n",
    "    The final layer uses Sigmoid activation: y=Sigmoid(holdW+b)y=Sigmoid(hold​W+b).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b6e7d07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_forward(x: np.ndarray, weights: list[np.ndarray], biases: list[np.ndarray]) -> tuple[np.ndarray, list[np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Computes the forward pass of the discriminator.\n",
    "    \n",
    "    Args:\n",
    "        x: Input data (batch_size, input_dim).\n",
    "        weights: List of weight matrices.\n",
    "        biases: List of bias vectors.\n",
    "        \n",
    "    Returns:\n",
    "        output: Probability of real (batch_size, 1).\n",
    "        activations: List of activations [x, h1, ..., output].\n",
    "    \"\"\"\n",
    "    \n",
    "    activations = [x]\n",
    "    h = x\n",
    "    slope = 0.2\n",
    "    N = len(weights)\n",
    "\n",
    "    for i in range(N):\n",
    "            h = h @ weights[i] + biases[i]\n",
    "\n",
    "            if i < N - 1:\n",
    "                # Leaky ReLU\n",
    "                h = np.where(h > 0, h, h * slope)\n",
    "            else:\n",
    "                # Sigmoid\n",
    "                h = 1 / (1 + np.exp(-h))\n",
    "\n",
    "            activations.append(h)\n",
    "    return h, activations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e879df",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "source": [
    "# Binary Cross Entropy (BCE) Loss\n",
    "\n",
    "The Binary Cross Entropy (BCE) loss function is widely used in Generative Adversarial Networks (GANs) for both the Generator and Discriminator.\n",
    "\n",
    "**Definition:**\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = - \\frac{1}{N} \\sum_{i=1}^{N} \\Big[ y_i \\log(\\hat{y}_i + \\epsilon) + (1 - y_i) \\log(1 - \\hat{y}_i + \\epsilon) \\Big]\n",
    "$$\n",
    "\n",
    "**Where:**\n",
    "\n",
    "- \\(N\\) is the batch size  \n",
    "- \\(y_i\\) is the true label, typically 0 or 1  \n",
    "- \\(\\hat{y}_i\\) is the predicted probability from the discriminator, in the range [0, 1]  \n",
    "- \\(\\epsilon = 1 \\times 10^{-8}\\) ensures numerical stability to avoid \\(\\log(0)\\)  \n",
    "\n",
    "**Notes:**\n",
    "\n",
    "- The loss is averaged over the batch.  \n",
    "- For GANs, the discriminator typically outputs \\(\\hat{y}_i\\) through a sigmoid activation to represent probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d8379800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce_loss(y_pred: np.ndarray, y_true: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Computes Binary Cross Entropy loss.\n",
    "    \n",
    "    Args:\n",
    "        y_pred: Predicted probabilities (batch_size, 1).\n",
    "        y_true: True labels (batch_size, 1).\n",
    "        \n",
    "    Returns:\n",
    "        loss: Scalar mean loss.\n",
    "    \"\"\" \n",
    "    epsilon = 1e-8\n",
    "    loss = y_true * np.log(y_pred + epsilon) + (1 - y_true) * np.log(1 - y_pred + epsilon)\n",
    "    return float(-np.mean(loss))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de6a675",
   "metadata": {},
   "source": [
    "## Binary Cross Entropy Loss (Backward)\n",
    "\n",
    "Implement the backward pass (gradient) of the Binary Cross Entropy loss function with respect to the predicted probabilities `y_pred`.\n",
    "\n",
    "---\n",
    "\n",
    "### Formula\n",
    "\n",
    "Given the loss:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = -\\frac{1}{N} \\sum_{i=1}^{N}\n",
    "\\left[\n",
    "y_i \\log(\\hat{y}_i + \\epsilon)\n",
    "+ (1 - y_i)\\log(1 - \\hat{y}_i + \\epsilon)\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "The gradient with respect to $\\hat{y}_i$ is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}_i}\n",
    "=\n",
    "\\frac{1}{N}\n",
    "\\left(\n",
    "\\frac{1 - y_i}{1 - \\hat{y}_i + \\epsilon}\n",
    "-\n",
    "\\frac{y_i}{\\hat{y}_i + \\epsilon}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Where\n",
    "\n",
    "- $N$ is the batch size  \n",
    "- $\\epsilon = 1 \\times 10^{-8}$ for numerical stability  \n",
    "\n",
    "---\n",
    "\n",
    "### Inputs\n",
    "\n",
    "- `y_pred`: Predicted probabilities of shape `(batch_size, 1)`\n",
    "- `y_true`: True labels of shape `(batch_size, 1)`\n",
    "\n",
    "---\n",
    "\n",
    "### Output\n",
    "\n",
    "- `grad`: The gradient array of shape `(batch_size, 1)`, representing\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "97aea704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce_loss_backward(y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the gradient of BCE loss with respect to y_pred.\n",
    "    \n",
    "    Args:\n",
    "        y_pred: Predicted probabilities (batch_size, 1).\n",
    "        y_true: True labels (batch_size, 1).\n",
    "        \n",
    "    Returns:\n",
    "        grad: Gradient dL/dy_pred (batch_size, 1).\n",
    "    \"\"\"\n",
    "\n",
    "    epsilon = 1e-8\n",
    "    N = len(y_pred)\n",
    "    grad = (1/N) * (((1-y_true)/(1-y_pred + epsilon)) - ((y_true)/(y_pred + epsilon))) \n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6a9595",
   "metadata": {},
   "source": [
    "## Generator Backward Pass (Linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89425e73",
   "metadata": {},
   "source": [
    "Implement the backward pass (backpropagation) for the Generator network.\n",
    "\n",
    "---\n",
    "\n",
    "### Inputs\n",
    "\n",
    "- `d_output`: The gradient of the loss with respect to the generator's output,  \n",
    "  $$\n",
    "  \\frac{\\partial \\mathcal{L}}{\\partial h_N}\n",
    "  $$  \n",
    "  Shape `(batch_size, out_dim)`.\n",
    "\n",
    "- `activations`: A list of activations  \n",
    "  $$\n",
    "  [h_0, h_1, \\dots, h_N]\n",
    "  $$  \n",
    "  computed during the forward pass.\n",
    "\n",
    "- `weights`: List of weight matrices  \n",
    "  $$\n",
    "  [W_0, \\dots, W_{N-1}]\n",
    "  $$\n",
    "\n",
    "- `biases`: List of bias vectors  \n",
    "  $$\n",
    "  [b_0, \\dots, b_{N-1}]\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "### Architecture Reminder\n",
    "\n",
    "- Layers $0$ to $N-2$: ReLU activation  \n",
    "- Layer $N-1$ (last): Tanh activation\n",
    "\n",
    "---\n",
    "\n",
    "### Derivatives\n",
    "\n",
    "- **Tanh Derivative**:  \n",
    "  If  \n",
    "  $$\n",
    "  y = \\tanh(x)\n",
    "  $$  \n",
    "  then  \n",
    "  $$\n",
    "  y' = 1 - y^2\n",
    "  $$\n",
    "\n",
    "- **ReLU Derivative**:  \n",
    "  If  \n",
    "  $$\n",
    "  y = \\mathrm{ReLU}(x)\n",
    "  $$  \n",
    "  then  \n",
    "  $$\n",
    "  y' =\n",
    "  \\begin{cases}\n",
    "  1 & \\text{if } x > 0 \\\\\n",
    "  0 & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "  $$\n",
    "  (Strictly, if $y > 0$)\n",
    "\n",
    "---\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "Iterate backwards from the last layer $l = N-1$ down to $0$.\n",
    "\n",
    "#### Compute delta\n",
    "\n",
    "- **Last layer**:\n",
    "  $$\n",
    "  \\delta_N = d\\_output \\odot (1 - h_N^2)\n",
    "  $$\n",
    "\n",
    "- **Hidden layers**:\n",
    "  $$\n",
    "  \\delta_{l+1} = (\\delta_{l+2} W_{l+1}^T) \\odot (h_{l+1} > 0)\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "### Compute Gradients\n",
    "\n",
    "- **Weight gradients**:\n",
    "  $$\n",
    "  dW_l = h_l^T \\delta_{l+1}\n",
    "  $$\n",
    "\n",
    "- **Bias gradients**:\n",
    "  $$\n",
    "  db_l = \\sum \\delta_{l+1}\n",
    "  $$\n",
    "  (sum over the batch dimension)\n",
    "\n",
    "---\n",
    "\n",
    "### Output\n",
    "\n",
    "Returns a tuple `(grads_w, grads_b)`:\n",
    "\n",
    "- `grads_w`: List of weight gradients, same shape as `weights`\n",
    "- `grads_b`: List of bias gradients, same shape as `biases`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b1b767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generator_backward(d_output: np.ndarray, activations: list[np.ndarray], weights: list[np.ndarray], biases: list[np.ndarray]) -> tuple[list[np.ndarray], list[np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Computes gradients for generator weights and biases.\n",
    "    \n",
    "    Args:\n",
    "        d_output: Gradient dL/d(output), shape (batch_size, output_dim)\n",
    "        activations: List of activations [h0, h1, ..., output], where h0 is input z\n",
    "        weights: List of weight matrices, each shape (in_dim, out_dim)\n",
    "        biases: List of bias vectors, each shape (out_dim,)\n",
    "        \n",
    "    Returns:\n",
    "        grads_w: List of weight gradients\n",
    "        grads_b: List of bias gradients\n",
    "    \"\"\"\n",
    "    \n",
    "    N = len(weights)\n",
    "    delta = d_output * (1 - activations[-1]**2)  \n",
    "    grads_w = []\n",
    "    grads_b = []\n",
    "    \n",
    "    for i in reversed(range(N)):\n",
    "        grads_w.append(activations[i].T @ delta)  # (in_dim, out_dim)\n",
    "        grads_b.append(np.sum(delta, axis=0))     # make 1D\n",
    "        \n",
    "        if i > 0:\n",
    "            delta = (delta @ weights[i].T) * (activations[i] > 0)\n",
    "     \n",
    "    # Reverse to match input → output order\n",
    "    grads_w = grads_w[::-1]\n",
    "    grads_b = grads_b[::-1]\n",
    "    \n",
    "    return grads_w, grads_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b34537",
   "metadata": {},
   "source": [
    "# Discriminator Backward Pass (Linear)\n",
    "\n",
    "## Inputs\n",
    "- **d_output**: Gradient of the loss with respect to the discriminator output  \n",
    "  Shape: `(batch_size, 1)`\n",
    "- **activations**: List of activations `[h₀, h₁, ..., h_N]`\n",
    "- **weights**: List of weight matrices\n",
    "- **biases**: List of bias vectors\n",
    "\n",
    "---\n",
    "\n",
    "## Architecture\n",
    "- Layers `0` to `N−2`: **LeakyReLU** activation (slope = 0.2)\n",
    "- Layer `N−1` (last): **Sigmoid** activation\n",
    "\n",
    "---\n",
    "\n",
    "## Derivatives\n",
    "\n",
    "### Sigmoid\n",
    "If  \n",
    "\\[\n",
    "y = \\sigma(x)\n",
    "\\]\n",
    "\n",
    "Then  \n",
    "\\[\n",
    "y' = y(1 - y)\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### LeakyReLU\n",
    "If  \n",
    "\\[\n",
    "y = \\text{LeakyReLU}(x)\n",
    "\\]\n",
    "\n",
    "Then  \n",
    "\\[\n",
    "y' =\n",
    "\\begin{cases}\n",
    "1, & y \\ge 0 \\\\\n",
    "0.2, & y < 0\n",
    "\\end{cases}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## Backpropagation Algorithm\n",
    "\n",
    "Iterate backwards from layer `l = N−1` to `0`.\n",
    "\n",
    "### 1. Delta Computation\n",
    "\n",
    "- **Last layer**:\n",
    "\\[\n",
    "\\delta_N = d\\_output \\odot h_N (1 - h_N)\n",
    "\\]\n",
    "\n",
    "- **Hidden layers**:\n",
    "\\[\n",
    "\\delta_{l+1} = (\\delta_{l+2} W_{l+1}^T) \\odot \\text{LeakyReLU}'(h_{l+1})\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Gradient Computation\n",
    "\n",
    "- **Weights**:\n",
    "\\[\n",
    "dW_l = h_l^T \\delta_{l+1}\n",
    "\\]\n",
    "\n",
    "- **Biases**:\n",
    "\\[\n",
    "db_l = \\sum \\delta_{l+1}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## Output\n",
    "Returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134e16fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_backward(d_output: np.ndarray, activations: list[np.ndarray], weights: list[np.ndarray], biases: list[np.ndarray]) -> tuple[list[np.ndarray], list[np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Computes gradients for discriminator weights and biases.\n",
    "    \n",
    "    Args:\n",
    "        d_output: Gradient dL/d(output) (batch_size, 1).\n",
    "        activations: List of [x, h1, ..., output].\n",
    "        weights: List of weights.\n",
    "        biases: List of biases.\n",
    "        \n",
    "    Returns:\n",
    "        grads_w: List of weight gradients.\n",
    "        grads_b: List of bias gradients.\n",
    "    \"\"\"\n",
    "    N = len(weights)\n",
    "    delta = d_output * activations[-1] * (1 - activations[-1])\n",
    "    grads_w = []\n",
    "    grads_b = []\n",
    "    alpha = 0.2\n",
    "    \n",
    "    for i in reversed(range(N)):\n",
    "        grads_w.append(activations[i].T @ delta)  # (in_dim, out_dim)\n",
    "        grads_b.append(np.sum(delta, axis=0))     # make 1D\n",
    "        \n",
    "        if i > 0:\n",
    "            leaky_grad = np.where(activations[i] >= 0, 1.0, alpha)\n",
    "            delta = (delta @ weights[i].T) * leaky_grad \n",
    "    \n",
    "    # Reverse to match input → output order\n",
    "    grads_w = grads_w[::-1]\n",
    "    grads_b = grads_b[::-1]\n",
    "    \n",
    "    return grads_w, grads_b\n",
    "\n",
    "\n",
    "#  def discriminator_backward(activations: list[np.ndarray], y_true: np.ndarray, weights: list[np.ndarray], biases: list[np.ndarray]) -> tuple[list[np.ndarray], list[np.ndarray]]:\n",
    "#     \"\"\"\n",
    "#     Args:\n",
    "#         activations: List of [x, h1, ..., y_pred]\n",
    "#         y_true: True labels (batch_size, 1) - You must pass this in!\n",
    "#         weights: List of weights\n",
    "#         biases: List of biases\n",
    "#     \"\"\"\n",
    "#     N = len(weights)\n",
    "#     y_pred = activations[-1]\n",
    "#     batch_size = y_true.shape[0]\n",
    "\n",
    "#     # STABLE GRADIENT: (Prediction - Target) / Batch_Size\n",
    "#     # This combines BCE backward and Sigmoid backward into one step\n",
    "#     delta = (y_pred - y_true) / batch_size \n",
    "    \n",
    "#     grads_w = []\n",
    "#     grads_b = []\n",
    "#     alpha = 0.2\n",
    "    \n",
    "#     for i in reversed(range(N)):\n",
    "#         # Calculate gradients for this layer\n",
    "#         grads_w.append(activations[i].T @ delta)\n",
    "#         grads_b.append(np.sum(delta, axis=0))\n",
    "        \n",
    "#         if i > 0:\n",
    "#             # Backpropagate delta to the previous layer using LeakyReLU derivative\n",
    "#             leaky_grad = np.where(activations[i] >= 0, 1.0, alpha)\n",
    "#             delta = (delta @ weights[i].T) * leaky_grad\n",
    "    \n",
    "#     return grads_w[::-1], grads_b[::-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485f615c",
   "metadata": {},
   "source": [
    "# Minibatch Discrimination (GANs)\n",
    "\n",
    "Minibatch Discrimination is a technique used in GAN discriminators to reduce **mode collapse** by allowing the discriminator to consider **relationships between samples in a minibatch**, rather than evaluating each sample independently.\n",
    "\n",
    "---\n",
    "\n",
    "## Definitions\n",
    "\n",
    "- **N**: batch size  \n",
    "- **A**: input feature dimension  \n",
    "- **B**: number of kernels  \n",
    "- **C**: kernel dimension  \n",
    "\n",
    "---\n",
    "\n",
    "## Inputs\n",
    "\n",
    "- **features**  \n",
    "  Shape: `(N, A)`  \n",
    "  Activations from a discriminator layer.\n",
    "\n",
    "- **T**  \n",
    "  Shape: `(A, B, C)`  \n",
    "  Learnable transformation tensor.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Linear Projection\n",
    "\n",
    "We compute:\n",
    "\n",
    "$$\n",
    "M = f(x) \\cdot T\n",
    "$$\n",
    "\n",
    "Implementation details:\n",
    "\n",
    "- Reshape \\( T \\) from `(A, B, C)` to `(A, B \\cdot C)`\n",
    "- Perform matrix multiplication\n",
    "- Reshape the result to `(N, B, C)`\n",
    "\n",
    "Final shape:\n",
    "\n",
    "$$\n",
    "M \\in \\mathbb{R}^{N \\times B \\times C}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Step 2: Pairwise L1 Distances\n",
    "\n",
    "For each kernel \\( b \\in \\{1, \\dots, B\\} \\), compute the L1 distance between all samples:\n",
    "\n",
    "$$\n",
    "d_{i,j}^{(b)} = \\sum_{c=1}^{C} \\left| M_{i,b,c} - M_{j,b,c} \\right|\n",
    "$$\n",
    "\n",
    "This produces:\n",
    "\n",
    "$$\n",
    "d \\in \\mathbb{R}^{N \\times N \\times B}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Step 3: Exponential Similarity\n",
    "\n",
    "Convert distances into similarity scores:\n",
    "\n",
    "$$\n",
    "o_b(x_i) = \\sum_{j=1}^{N} \\exp\\left(-d_{i,j}^{(b)}\\right)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Step 4: Remove Self-Comparison (Optional)\n",
    "\n",
    "Since:\n",
    "\n",
    "$$\n",
    "d_{i,i}^{(b)} = 0 \\Rightarrow \\exp(0) = 1\n",
    "$$\n",
    "\n",
    "We subtract self-similarity:\n",
    "\n",
    "$$\n",
    "o_b(x_i) \\leftarrow o_b(x_i) - 1\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Output\n",
    "\n",
    "- **mb_features**  \n",
    "  Shape: `(N, B)`\n",
    "\n",
    "These features are typically concatenated with discriminator activations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3f3b9237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_discrimination(features, T):\n",
    "    \"\"\"\n",
    "    Computes minibatch discrimination features.\n",
    "    \n",
    "    Args:\n",
    "        features: (N, A)\n",
    "        T: (A, B, C)\n",
    "        \n",
    "    Returns:\n",
    "        mb_features: (N, B)\n",
    "    \"\"\"\n",
    "    N, A = features.shape\n",
    "    _, B, C = T.shape\n",
    "\n",
    "    # Step 1: Linear projection\n",
    "    T_flat = T.reshape(A, B * C)\n",
    "    M = features @ T_flat\n",
    "    M = M.reshape(N, B, C)\n",
    "\n",
    "    # Step 2: Pairwise L1 distances\n",
    "    M_i = M[:, None, :, :]          # (N, 1, B, C)\n",
    "    M_j = M[None, :, :, :]          # (1, N, B, C)\n",
    "    \n",
    "    distances = np.abs(M_i - M_j).sum(axis=3)  # (N, N, B)\n",
    "\n",
    "    # Step 3: Exponential similarity\n",
    "    mb_features = np.exp(-distances).sum(axis=1)\n",
    "\n",
    "    # Step 4: Remove self-comparison\n",
    "    mb_features -= 1\n",
    "\n",
    "    return mb_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e6781e",
   "metadata": {},
   "source": [
    "# WGAN-GP Gradient Penalty\n",
    "\n",
    "The **Gradient Penalty** used in **WGAN-GP (Wasserstein GAN with Gradient Penalty)** enforces the **1-Lipschitz constraint** on the discriminator (also called the *critic*).  \n",
    "Instead of weight clipping, WGAN-GP penalizes deviations of the gradient norm from 1.\n",
    "\n",
    "---\n",
    "\n",
    "## Intuition\n",
    "\n",
    "For the Wasserstein distance to be valid, the critic must be **1-Lipschitz**.  \n",
    "This means the gradient of the critic with respect to its input should have **unit norm everywhere**.\n",
    "\n",
    "The gradient penalty softly enforces this by penalizing gradients whose L2 norm is not close to 1.\n",
    "\n",
    "---\n",
    "\n",
    "## Formula\n",
    "\n",
    "## Definitions\n",
    "\n",
    "- Gradient penalty coefficient\n",
    "\n",
    "$$\n",
    "\\lambda = 10\n",
    "$$\n",
    "\n",
    "- Interpolated samples between real and generated data\n",
    "\n",
    "$$\n",
    "\\hat{x}\n",
    "$$\n",
    "\n",
    "- Critic output evaluated at interpolated samples\n",
    "\n",
    "$$\n",
    "D(\\hat{x})\n",
    "$$\n",
    "\n",
    "- Gradient of critic output with respect to interpolated input\n",
    "\n",
    "$$\n",
    "\\nabla_{\\hat{x}} D(\\hat{x})\n",
    "$$\n",
    "\n",
    "- L2 (Euclidean) norm\n",
    "\n",
    "$$\n",
    "\\lVert \\cdot \\rVert_2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Interpolated Samples\n",
    "\n",
    "The interpolated inputs are defined as:\n",
    "\n",
    "$$\n",
    "\\hat{x} = \\epsilon x_{\\text{real}} + (1 - \\epsilon) x_{\\text{fake}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "\\epsilon \\sim \\mathcal{U}(0, 1)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Inputs\n",
    "\n",
    "- **gradients**  \n",
    "  Shape: `(batch_size, input_dim)`  \n",
    "  Gradients of the critic output with respect to interpolated inputs.\n",
    "\n",
    "- **lambda_gp**  \n",
    "  Scalar \\( \\lambda \\) controlling penalty strength.\n",
    "\n",
    "---\n",
    "\n",
    "## Computation Steps\n",
    "\n",
    "1. Compute the L2 norm of each gradient:\n",
    "   $$\n",
    "   \\left\\| \\nabla_{\\hat{x}} D(\\hat{x}_i) \\right\\|_2\n",
    "   $$\n",
    "\n",
    "2. Subtract 1 from each norm:\n",
    "   $$\n",
    "   \\left\\| \\nabla_{\\hat{x}} D(\\hat{x}_i) \\right\\|_2 - 1\n",
    "   $$\n",
    "\n",
    "3. Square the result:\n",
    "   $$\n",
    "   \\left(\n",
    "   \\left\\| \\nabla_{\\hat{x}} D(\\hat{x}_i) \\right\\|_2 - 1\n",
    "   \\right)^2\n",
    "   $$\n",
    "\n",
    "4. Take the mean over the batch:\n",
    "   $$\n",
    "   \\mathbb{E}[\\cdot]\n",
    "   $$\n",
    "\n",
    "5. Multiply by \\( \\lambda \\).\n",
    "\n",
    "---\n",
    "\n",
    "## Output\n",
    "\n",
    "- **penalty**  \n",
    "  Scalar value representing the mean gradient penalty over the batch.\n",
    "\n",
    "---\n",
    "\n",
    "## Why This Works\n",
    "\n",
    "- Enforces the Lipschitz constraint smoothly\n",
    "- More stable than weight clipping\n",
    "- Improves convergence and training stability\n",
    "- Prevents exploding or vanishing gradients in the critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98138633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(gradients: np.ndarray, lambda_gp: float = 10.0) -> float:\n",
    "    \"\"\"\n",
    "    Computes WGAN-GP gradient penalty.\n",
    "\n",
    "    Args:\n",
    "        gradients: Gradients of D w.r.t interpolated inputs\n",
    "                   Shape: (batch_size, input_dim)\n",
    "        lambda_gp: Gradient penalty weight\n",
    "\n",
    "    Returns:\n",
    "        Scalar gradient penalty\n",
    "    \"\"\"\n",
    "\n",
    "    grad_norm = np.linalg.norm(gradients, axis=1)  # (batch_size,)\n",
    "\n",
    "    # Compute penalty\n",
    "    penalty = lambda_gp * np.mean((grad_norm - 1.0) ** 2)\n",
    "\n",
    "    return penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a188693",
   "metadata": {},
   "source": [
    "# Feature Matching Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738fd484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_matching_loss(real_features: np.ndarray, fake_features: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Computes feature matching loss.\n",
    "    \n",
    "    Args:\n",
    "        real_features: Features from real data (N, D).\n",
    "        fake_features: Features from fake data (M, D).\n",
    "    \n",
    "    Returns:\n",
    "        loss: Scalar squared L2 distance between means.\n",
    "    \"\"\"\n",
    "    \n",
    "    mu_real = np.mean(real_features, axis=0)\n",
    "    mu_fake = np.mean(fake_features, axis=0)\n",
    "    loss = np.sum(np.square(mu_real - mu_fake))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a8ba60",
   "metadata": {},
   "source": [
    "# Hyper Parameters Definition\n",
    "\n",
    "- batch size usually between 32-128\n",
    "- learning rates usually ~ 1e-4\n",
    "- num_epocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b6c1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 100\n",
    "learning_rate_g = 0.0002\n",
    "learning_rate_d = 0.0003\n",
    "batch_size = 64\n",
    "num_epochs = 10\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
